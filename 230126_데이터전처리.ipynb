{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d08a2177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "75\n",
      "--------------------\n",
      "(105, 4)\n",
      "Iris-virginica     39\n",
      "Iris-versicolor    33\n",
      "Iris-setosa        33\n",
      "Name: target, dtype: int64\n",
      "----------\n",
      "(45, 4)\n",
      "Iris-versicolor    17\n",
      "Iris-setosa        17\n",
      "Iris-virginica     11\n",
      "Name: target, dtype: int64\n",
      "--------------------\n",
      "(105, 4)\n",
      "Iris-virginica     35\n",
      "Iris-setosa        35\n",
      "Iris-versicolor    35\n",
      "Name: target, dtype: int64\n",
      "----------\n",
      "(45, 4)\n",
      "Iris-virginica     15\n",
      "Iris-setosa        15\n",
      "Iris-versicolor    15\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 1. 표본추출\n",
    "df = pd.read_csv('data/iris.csv')\n",
    "# 1)단순임의추출(복원, 비복원) : 각 원소(개체)에 번호를 부여하여, 임의적으로 n개를 추출하는 방법으로 모든 원소(개체)는 선택될 확률이 동일.\n",
    "# 1-1) 단순임의추출 (n : 개수추출, frac : 비율추출, replace = True : 복원추출) \n",
    "df_simple = df.sample(frac = 0.5, replace = True, random_state = 2023)\n",
    "print(len(df))\n",
    "print(len(df_simple))\n",
    "print('--------------------')\n",
    "\n",
    "\n",
    "\n",
    "# 1-2) 독립변수/종속변수로 분리하여 단순임의추출을 진행하는 경우\n",
    "X = df.drop(columns = ['target'])\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 2023)\n",
    "print(X_train.shape)\n",
    "print(y_train.value_counts())\n",
    "print('----------')\n",
    "print(X_test.shape)\n",
    "print(y_test.value_counts())\n",
    "print('--------------------')\n",
    "\n",
    "\n",
    "\n",
    "# 2) 층화임의추출법 : 모집단이 이질적인 몇 개의 계층으로 이루어져 있을 때 모든 계층으로부터 원소(개체)를 임의로 추출하여,\n",
    "#                 각 계층을 고루 대표할 수 있도록 임의적으로 표본을 추출하는 방법.(shuffle = True : default, 추출 전에 데이터를 섞음)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify = y, random_state = 2023)\n",
    "print(X_train.shape)\n",
    "print(y_train.value_counts())\n",
    "print('----------')\n",
    "print(X_test.shape)\n",
    "print(y_test.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d7af44d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20) (1000,)\n",
      "1    700\n",
      "0    300\n",
      "Name: credit.rating, dtype: int64\n",
      "----------\n",
      "(700, 20) (700,)\n",
      "1    490\n",
      "0    210\n",
      "Name: credit.rating, dtype: int64\n",
      "(300, 20) (300,)\n",
      "1    210\n",
      "0     90\n",
      "Name: credit.rating, dtype: int64\n",
      "----------\n",
      "(980, 20) (980,)\n",
      "1    490\n",
      "0    490\n",
      "Name: credit.rating, dtype: int64\n",
      "----------\n",
      "(980, 20) (980,)\n",
      "1    490\n",
      "0    490\n",
      "Name: credit.rating, dtype: int64\n",
      "----------\n",
      "(420, 20) (420,)\n",
      "0    210\n",
      "1    210\n",
      "Name: credit.rating, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 2. 샘플링\n",
    "# 2-1) 오버샘플링\n",
    "# 마이너 클래스의 샘플을 증가시켜 메이저 클래스의 샘플 크기를 동일하게 만드는 방법.\n",
    "# 빅데이터 분석을 위해서는 많은 데이터 확보가 효과적이므로 오버샘플링 기법을 적용 추천.\n",
    "# 2-1-1) SMOTE(Synthetic Minority Over-sampling Technique)\n",
    "#  > 소수 레이블을 지닌 데이터 세트의 관측 값에 대한 K개의 최근접 이웃을 찾고,\n",
    "#  > 관측값과 이웃으로 선택된 값 사이에 새로운 데이터를 생성하는 방법으로 샘플의 수를 늘리는 방법.\n",
    "#  > 새로운 데이터나 작은 데이터셋에서는 사용하기가 어려움.\n",
    "import pandas as pd\n",
    "df = pd.read_csv('data/credit_final.csv')\n",
    "X = df.drop(['credit.rating'], axis = 1)\n",
    "y = df['credit.rating']\n",
    "print(X.shape, y.shape)\n",
    "print(y.value_counts()) # 0 : 300, 1 : 700\n",
    "print('----------')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify = y, random_state = 2023)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(y_train.value_counts())\n",
    "print(X_test.shape, y_test.shape)\n",
    "print(y_test.value_counts())\n",
    "print('----------')\n",
    "\n",
    "#!pip3 install imblearn > imbalance learn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(k_neighbors = 5, sampling_strategy = 'minority')\n",
    "X_resample, y_resample = smote.fit_resample(X_train, y_train)\n",
    "print(X_resample.shape,y_resample.shape)\n",
    "print(y_resample.value_counts()) # y_train 보다 전체 데이터 개수가 280개 늘어남(0 클래스가 280개 늘어남)\n",
    "print('----------')\n",
    "\n",
    "# 2-1-2) Random Over Sampling \n",
    "#  > 소수의 레이블을 지닌 데이터세트를 단순 복제하여 다수의 레이블과 비율을 맞추는 방법.\n",
    "#  > 랜덤 오버샘플링은 동일한 정보를 복사하여 과적합을 유발 가능성 존재.\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(sampling_strategy = 'minority')\n",
    "X_resample, y_resample = ros.fit_resample(X_train, y_train)\n",
    "print(X_resample.shape, y_resample.shape)\n",
    "print(y_resample.value_counts())\n",
    "print('----------')\n",
    "\n",
    "# 2-2) 언더샘플링\n",
    "# 다수의 레이블을 가진 데이터를 샘플링하여 소수의 데이터세트를 가진 레이블의 수준으로 감소(데이터 손실 유발).\n",
    "# 데이터 불균형 문제는 해소 가능하나, 학습 성능의 저하 가능성(과소적합) 존재.\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(sampling_strategy = 'majority') # majority : 다수 레이블의 데이터를 샘플링\n",
    "X_resample, y_resample = rus.fit_resample(X_train, y_train)\n",
    "print(X_resample.shape, y_resample.shape)\n",
    "print(y_resample.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c1826b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5376526277146434e-17   1.000715051932627\n",
      "0.0005735068871783522   1.021074396285328\n",
      "----------\n",
      "0.5914285714285714   0.4154535784699398\n",
      "0.5916666666666667   0.4239058970898069\n"
     ]
    }
   ],
   "source": [
    "# 3. 데이터 표준화, 정규화\n",
    "# 3-1) 데이터 표준화 또는 정규화가 필요한 이유\n",
    "# > 차원마다 단위, 평균, 표준편차, 범위가 다르다면 우리가 학습시키는 머신이 그 차이를 인지하지 못함.\n",
    "# > 통계는 표본집단의 평균과 분산으로 모집단의 평균과 분산을 설명하는 학문인데 차원마다 단위, 평균, 분산, 범위가 다르면\n",
    "# > 스케일이 큰 집단에 영향을 많이 받음.\n",
    "# > 즉, 우리는 머신에 학습을 시킬 때 각 차원을 유사한 단위, 평균, 분산, 범위로 맞춰 줘야 함.\n",
    "\n",
    "# 3-2) StandardScaler(표준화)\n",
    "# > 평균 0, 분산 1인 표준정규분포로 모든 데이터를 변환\n",
    "# > 최솟값과 최댓값의 크기를 제한하지 않아 이상치에 민감 > 이상치 미리 확인 및 정제 필요\n",
    "# > 분류분석에 유용\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('data/credit_final.csv')\n",
    "X = df.drop(['credit.rating'], axis = 1)\n",
    "y = df['credit.rating']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify = y, random_state = 2023)\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_train_scaled = ss.fit_transform(X_train)\n",
    "X_test_scaled = ss.transform(X_test) # train 데이터와 test 데이터를 같은 scaler 객체 사용 필요.\n",
    "\n",
    "x_train_scaled_df = pd.DataFrame(X_train_scaled)\n",
    "x_test_scaled_df = pd.DataFrame(X_test_scaled)\n",
    "print(x_train_scaled_df[0].mean(), ' ', x_train_scaled_df[0].std() )\n",
    "print(x_test_scaled_df[0].mean(), ' ' , x_test_scaled_df[0].std() )\n",
    "print('----------')\n",
    "\n",
    "# 3-3) MinMaxScaler : 정규화, 최대/최소값이 1, 0이 되도록 데이터를 변환\n",
    "# 이상치에 민감 > 이상치 미리 확인 및 정제 필요\n",
    "# 회귀분석에 유용\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mms = MinMaxScaler()\n",
    "X_train_scaled = mms.fit_transform(X_train)\n",
    "X_test_scaled = mms.transform(X_test) # train 데이터와 test 데이터를 같은 scaler 객체 사용 필요.\n",
    "\n",
    "x_train_scaled_df = pd.DataFrame(X_train_scaled)\n",
    "x_test_scaled_df = pd.DataFrame(X_test_scaled)\n",
    "print(x_train_scaled_df[0].mean(), ' ', x_train_scaled_df[0].std() )\n",
    "print(x_test_scaled_df[0].mean(), ' ' , x_test_scaled_df[0].std() )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
